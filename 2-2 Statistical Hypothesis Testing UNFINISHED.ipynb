{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author:            Simon Zahn\n",
      "Last Updated:      2019-08-04 16:10\n",
      "Python Version:    3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Notebook Info\n",
    "nb_info = {'Author':'Simon Zahn', 'Last Updated':dt.datetime.now().strftime('%Y-%m-%d %H:%M'), 'Python Version':sys.version }\n",
    "\n",
    "for k,v in nb_info.items():\n",
    "    print((k + ':').ljust(18), str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Imports and Top Matter](#Imports-and-Top-Matter)\n",
    "1. [Statistical Hypothesis Testing](#Statistical-Hypothesis-Testing)\n",
    "1. [Interpreting the p-value](#Interpreting-the-p-value)\n",
    "1. [Interpreting Critical Values](#Interpreting-Critical-Values)\n",
    "1. [Errors in Statistical Tests](#Errors-in-Statistical-Tests)\n",
    "1. [Examples of Hypothesis Tests](#Examples-of-Hypothesis-Tests)\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Top Matter\n",
    "[[back to top]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Materials summarized from [A Gentle Introduction to Statistical Hypothesis Testing](#https://machinelearningmastery.com/statistical-hypothesis-tests/) on Machine Learning Mastery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "# general\n",
    "\n",
    "# IPython\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, special\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pal = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Hypothesis Testing\n",
    "[[back to top]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data alone is not interesting. It is the interpretation of the data that we are really interested in. Two concrete examples that we will use a lot in machine learning are:\n",
    "\n",
    "- A test that assumes that data has a normal distribution.\n",
    "- A test that assumes that two samples were drawn from the same underlying population distribution.\n",
    "\n",
    "The assumption of a statistical test is called the null hypothesis, $H_0$. It is often called the default assumption, or the assumption that nothing has changed. A violation of the test’s assumption is called the alternate hypothesis or $H_1$. \n",
    "\n",
    "- **Null Hypothesis** ($H_0$): Assumption of the test holds and is failed to be rejected at some level of significance.\n",
    "- **Alternate Hypothesis** ($H_1$): Assumption of the test does not hold and is rejected at some level of significance.\n",
    "\n",
    "Before we can reject or fail to reject the null hypothesis, we must interpret the result of the test. The results of a statistical hypothesis test must be interpreted for us to start making claims. This is a point that may cause a lot of confusion for beginners and experienced practitioners alike. There are two common forms that a result from a statistical hypothesis test may take, and they must be interpreted in different ways. They are the p-value and critical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the p-value\n",
    "[[back to top]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We describe a finding as statistically significant by interpreting the p-value. For example, we may perform a normality test on a data sample and find that it is unlikely that sample of data deviates from a Gaussian distribution, failing to reject the null hypothesis. \n",
    "\n",
    "The p-value is a quantity that we can use to interpret or quantify the result of the test and either reject or fail to reject the null hypothesis. This is done by comparing the p-value to a threshold value chosen beforehand called the significance level. The significance level is often referred to by the Greek lower case letter alpha. A common value used for $\\alpha$ is 5% or 0.05. A smaller $\\alpha$ value suggests a more robust interpretation of the null hypothesis, such as 1% or 0.1%. The p-value is compared to the pre-chosen $\\alpha$ value. A result is statistically significant when the p-value is less than $\\alpha$. This signifies a change was detected: that the default hypothesis can be rejected.\n",
    "\n",
    "- If p-value > $\\alpha$: Fail to reject the null hypothesis (i.e. not signifiant result).\n",
    "- If p-value <= $\\alpha$: Reject the null hypothesis (i.e. significant result).\n",
    "\n",
    "For example, if we were performing a test of whether a data sample was normal and we calculated a p-value of .07, we could state something like: The test found that the data sample was normal, failing to reject the null hypothesis at a 5% significance level. The significance level can be inverted by subtracting it from 1 to give a confidence level of the hypothesis given the observed sample data.\n",
    "\n",
    "\n",
    "confidence level = 1 - significance level\n",
    "\n",
    "Therefore, statements such as the following can also be made: The test found that the data was normal, failing to reject the null hypothesis at a 95% confidence level.\n",
    "\n",
    "##### “Reject” vs “Failure to Reject”\n",
    "The p-value is probabilistic. This means that when we interpret the result of a statistical test, we do not know what is true or false, only what is likely. Rejecting the null hypothesis means that there is sufficient statistical evidence that the null hypothesis does not look likely. Otherwise, it means that there is not sufficient statistical evidence to reject the null hypothesis.\n",
    "\n",
    "We may think about the statistical test in terms of the dichotomy of rejecting and accepting the null hypothesis. The danger is that if we say that we “accept” the null hypothesis, the language suggests that the null hypothesis is true. Instead, it is safer to say that we “fail to reject” the null hypothesis, as in, there is insufficient statistical evidence to reject it.\n",
    "\n",
    "##### Common p-value Misinterpretations\n",
    "- *True or False Null Hypothesis*: The interpretation of the p-value does not mean that the null hypothesis is true or false. It does mean that we have chosen to reject or fail to reject the null hypothesis at a specific statistical significance level based on empirical evidence and the chosen statistical test. You are limited to making probabilistic claims, not crisp binary or true/false claims about the result.\n",
    "\n",
    "- *p-value as Probability*: A common misunderstanding is that the p-value is a probability of the null hypothesis being true or false given the data. In probability, this would be written as follows: P(hypothesis | data). Instead, the p-value can be thought of as the probability of the data given the pre-specified assumption embedded in the statistical test, which would be written as: Pr(data | hypothesis). It allows us to reason about whether or not the data fits the hypothesis. Not the other way around. The p-value is a measure of how likely the data sample would be observed if the null hypothesis were true.\n",
    "\n",
    "- *Post-Hoc Tuning*: It does not mean that you can re-sample your domain or tune your data sample and re-run the statistical test until you achieve a desired result. It also does not mean that you can choose your p-value after you run the test. This is called p-hacking or hill climbing and will mean that the result you present will be fragile and not representative. In science, this is at best unethical, and at worst fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Critical Values\n",
    "[[back to top]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tests do not return a p-value. Instead, they might return a list of critical values and their associated significance levels, as well as a test statistic. These are usually nonparametric or distribution-free statistical hypothesis tests. The choice of returning a p-value or a list of critical values is really an implementation choice.\n",
    "\n",
    "The results are interpreted in a similar way. Instead of comparing a single p-value to a pre-specified significance level, the test statistic is compared to the critical value at a chosen significance level.\n",
    "\n",
    "- If test statistic < critical value: Fail to reject the null hypothesis.\n",
    "- If test statistic >= critical value: Reject the null hypothesis.\n",
    "\n",
    "Again, the meaning of the result is similar in that the chosen significance level is a probabilistic decision on rejection or fail to reject the base assumption of the test given the data.\n",
    "\n",
    "Results are presented in the same way as with a p-value, as either significance level or confidence level. For example, if a normality test was calculated and the test statistic was compared to the critical value at the 5% significance level, results could be stated as:\n",
    "\n",
    "- The test found that the data sample was normal, failing to reject the null hypothesis at a 5% significance level.\n",
    "- The test found that the data was normal, failing to reject the null hypothesis at a 95% confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors in Statistical Tests\n",
    "[[back to top]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of a statistical hypothesis test is probabilistic. That means that the evidence of the test may suggest an outcome and be mistaken. For example, if $\\alpha$ was 5%, it suggests that (at most) 1 time in 20 that the null hypothesis would be mistakenly rejected or failed to be rejected because of the statistical noise in the data sample.\n",
    "\n",
    "Given a small p-value (reject the null hypothesis) either means that the null hypothesis false (we got it right) or it is true and some rare and unlikely event has been observed (we made a mistake). If this type of error is made, it is called a *false positive*. We falsely believe the rejection of the null hypothesis.\n",
    "\n",
    "Alternately, given a large p-value (fail to reject the null hypothesis), it may mean that the null hypothesis is true (we got it right) or that the null hypothesis is false and some unlikely event occurred (we made a mistake). If this type of error is made, it is called a *false negative*. We falsely believe the null hypothesis or assumption of the statistical test.\n",
    "\n",
    "Each of these two types of error has a specific name.\n",
    "\n",
    "- **Type I Error**: The incorrect rejection of a true null hypothesis or a false positive.\n",
    "- **Type II Error**: The incorrect failure of rejection of a false null hypothesis or a false negative.\n",
    "\n",
    "All statistical hypothesis tests have a chance of making either of these types of errors. False findings or false disoveries are more than possible; they are probable. Ideally, we want to choose a significance level that minimizes the likelihood of one of these errors. E.g. a very small significance level. Although significance levels such as 0.05 and 0.01 are common in many fields of science, harder sciences, such as physics, are more aggressive.\n",
    "\n",
    "It is common to use a significance level of $3 * 10^{-7}$ or $0.0000003$, often referred to as $5\\sigma$. This means that the finding was due to chance with a probability of 1 in 3.5 million independent repeats of the experiments. To use a threshold like this may require a much large data sample.Nevertheless, these types of errors are always present and must be kept in mind when presenting and interpreting the results of statistical tests. It is also a reason why it is important to have findings independently verified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Hypothesis Tests\n",
    "[[back to top]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of statistical hypothesis tests. This section lists some common examples of statistical hypothesis tests and the types of problems that they are used to address:\n",
    "\n",
    "**Variable Distribution Type Tests (Gaussian)**\n",
    "- Shapiro-Wilk Test\n",
    "- D’Agostino’s K^2 Test\n",
    "- Anderson-Darling Test\n",
    "\n",
    "**Variable Relationship Tests (correlation)**\n",
    "- Pearson’s Correlation Coefficient\n",
    "- Spearman’s Rank Correlation\n",
    "- Kendall’s Rank Correlation\n",
    "- Chi-Squared Test\n",
    "\n",
    "**Compare Sample Means (parametric)**\n",
    "- Student’s t-test\n",
    "- Paired Student’s t-test\n",
    "- Analysis of Variance Test (ANOVA)\n",
    "- Repeated Measures ANOVA Test\n",
    "\n",
    "**Compare Sample Means (nonparametric)**\n",
    "- Mann-Whitney U Test\n",
    "- Wilcoxon Signed-Rank Test\n",
    "- Kruskal-Wallis H Test\n",
    "- Friedman Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Normality Testing in Python\n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Parametric Stat Testing in Python\n",
    "https://machinelearningmastery.com/parametric-statistical-significance-tests-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Nonparametric Stat Testing in Python\n",
    "https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Correlation Testing in Python\n",
    "https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/\n",
    "https://machinelearningmastery.com/how-to-calculate-nonparametric-rank-correlation-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
